#!/usr/bin/env python3

import os
import sys
import time
import json
import base64
import logging
import signal
import subprocess
import threading
import re
from datetime import datetime
from queue import Queue
import cv2
import numpy as np
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal, QObject
from PyQt5.QtGui import QImage, QPixmap, QFont, QIcon
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QLabel,
                             QVBoxLayout, QHBoxLayout, QPushButton, QTextEdit,
                             QDesktopWidget, QSpacerItem, QSizePolicy)
import requests
# å®‰å…¨åœ°å¯¼å…¥é…ç½®,å¦‚æœå¤±è´¥åˆ™æ‰“å°æ˜ç¡®é”™è¯¯å¹¶é€€å‡º
try:
    from config import *
except ImportError as e:
    print("=" * 60)
    print("!!! è‡´å‘½é”™è¯¯: æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ 'config.py' !!!")
    print(f"é”™è¯¯è¯¦æƒ…: {e}")
    print("è¯·ç¡®ä¿ 'config.py' æ–‡ä»¶å­˜åœ¨äºåŒä¸€ç›®å½•ä¸‹,å¹¶ä¸”æ²¡æœ‰è¯­æ³•é”™è¯¯ã€‚")
    print("=" * 60)
    sys.exit(1)
# æ–°å¢:å®šæ—¶åˆ†æçš„é…ç½®
# å»ºè®®å°†æ­¤å˜é‡ç§»è‡³ config.py æ–‡ä»¶ä¸­è¿›è¡Œç»Ÿä¸€ç®¡ç†
TIMED_ANALYSIS_INTERVAL = 120  # å®šæ—¶åˆ†æçš„é—´éš”æ—¶é—´(ç§’)
class AIFitnessApp(QMainWindow):
    """
    ä¸»åº”ç”¨çª—å£ç±»,æ•´åˆäº† GUI å’Œ AI åç«¯é€»è¾‘ã€‚
    """
    # å®šä¹‰ä¸€ä¸ªä¿¡å·,ç”¨äºåœ¨é GUI çº¿ç¨‹å®Œæˆåˆ†æåå®‰å…¨åœ°æ›´æ–° GUI
    analysis_complete_signal = pyqtSignal(str)
    def __init__(self):
        super().__init__()
        # ------------------- AI åç«¯åˆå§‹åŒ– (åŠŸèƒ½æœªä¿®æ”¹) -------------------
        self.image_path = IMAGE_PATH
        self.vision_api_url = VISION_API_URL
        self.analysis_prompt = ANALYSIS_PROMPT
        self.camera_index = CAMERA_INDEX
        self.llm_output_file = LLM_OUTPUT_FILE
        self.summary_output_file = SUMMARY_OUTPUT_FILE
        self.tts_bin = TTS_BIN
        self.audio_output_dir = AUDIO_OUTPUT_DIR
        self.output_audio_file = os.path.join(self.audio_output_dir, "audio.wav")
        self.voice_output_file = '/tmp/voice_recognition_output.txt'
        self.quick_commands = QUICK_COMMANDS
        self.running = True
        self.exit_requested = False
        self.current_frame = None
        self.frame_lock = threading.Lock()
        self.setup_logging()
        os.makedirs(self.audio_output_dir, exist_ok=True)
        self.analysis_complete_signal.connect(self.handle_analysis_result)
        # ------------------- GUI åˆå§‹åŒ– (ç•Œé¢ç¾åŒ–) -------------------
        self.setWindowTitle("â—‡ æ™ºèƒ½å¥èº«åŠ©æ‰‹")
        screen = QDesktopWidget().screenGeometry()
        self.screen_width = screen.width()
        self.screen_height = screen.height()
        self.setGeometry(0, 0, self.screen_width, self.screen_height) # ä½¿ç”¨setGeometryä»¥é€‚åº”å…¨å±
        # è®¾ç½®å…¨å±€å­—ä½“
        font = QFont("Microsoft YaHei UI", 10)
        QApplication.setFont(font)
        self.camera = cv2.VideoCapture(self.camera_index)
        if not self.camera.isOpened():
            self.logger.critical(f"æ— æ³•æ‰“å¼€æ‘„åƒå¤´,ç´¢å¼•: {self.camera_index}")
            sys.exit(1)
        self.main_widget = QWidget()
        self.setCentralWidget(self.main_widget)
        # ä¸»å¸ƒå±€: æ°´å¹³å¸ƒå±€
        main_layout = QHBoxLayout(self.main_widget)
        main_layout.setContentsMargins(15, 15, 15, 15)
        main_layout.setSpacing(15)
        # --- å·¦ä¾§å¸ƒå±€ (æ‘„åƒå¤´å’ŒæŒ‰é’®) ---
        left_container = QWidget()
        left_layout = QVBoxLayout(left_container)
        left_layout.setContentsMargins(0, 0, 0, 0)
        left_layout.setSpacing(15)
        self.camera_label = QLabel()
        self.camera_label.setAlignment(Qt.AlignCenter)
        left_layout.addWidget(self.camera_label, 1)
        button_container = QWidget()
        button_layout = QHBoxLayout(button_container)
        button_layout.setContentsMargins(0,0,0,0)
        button_layout.setSpacing(15)
        button_layout.addSpacerItem(QSpacerItem(40, 20, QSizePolicy.Expanding, QSizePolicy.Minimum))
        self.analyze_now_button = QPushButton("â—‡ è¯­éŸ³åˆ†æ")
        self.analyze_now_button.setObjectName("AnalyzeButton")
        self.analyze_now_button.clicked.connect(self.trigger_manual_analysis)
        button_layout.addWidget(self.analyze_now_button)
        self.stop_button = QPushButton("â—‡ åœæ­¢åˆ†æ")
        self.stop_button.setObjectName("StopButton")
        self.stop_button.clicked.connect(self.stop_ai_services)
        button_layout.addWidget(self.stop_button)
        button_layout.addSpacerItem(QSpacerItem(40, 20, QSizePolicy.Expanding, QSizePolicy.Minimum))
        left_layout.addWidget(button_container)
        # --- å³ä¾§å¸ƒå±€ (ä¿¡æ¯é¢æ¿) ---
        right_container = QWidget()
        right_layout = QVBoxLayout(right_container)
        right_layout.setContentsMargins(0, 0, 0, 0)
        right_layout.setSpacing(15)
        self.analysis_container = QWidget()
        self.analysis_container.setObjectName("InfoPanel")
        analysis_layout = QVBoxLayout(self.analysis_container)
        analysis_title = QLabel("â—‡ å®æ—¶ AI åˆ†æ")
        analysis_title.setObjectName("PanelTitle")
        analysis_layout.addWidget(analysis_title)

        # MODIFICATION: å°† QLabel æ›´æ¢ä¸º QTextEdit ä»¥æ”¯æŒæ»šåŠ¨æ¡
        self.analysis_text = QTextEdit("æ­£åœ¨å¯åŠ¨é¦–æ¬¡è‡ªåŠ¨åˆ†æ...")
        self.analysis_text.setReadOnly(True)
        analysis_layout.addWidget(self.analysis_text, 1)

        self.summary_container = QWidget()
        self.summary_container.setObjectName("InfoPanel")
        summary_layout = QVBoxLayout(self.summary_container)
        summary_title = QLabel("â—‡ è®­ç»ƒå†å²åŠç»ƒåæ€»ç»“")
        summary_title.setObjectName("PanelTitle")
        summary_layout.addWidget(summary_title)
        self.summary_text = QTextEdit("è®­ç»ƒç»“æŸåï¼Œæœ€ç»ˆç”Ÿæˆçš„æ€»ç»“æŠ¥å‘Šå°†æ˜¾ç¤ºåœ¨æ­¤å¤„ã€‚")
        self.summary_text.setReadOnly(True)
        summary_layout.addWidget(self.summary_text, 1)
        right_layout.addWidget(self.analysis_container, 1)
        right_layout.addWidget(self.summary_container, 1)
        main_layout.addWidget(left_container, 3)
        main_layout.addWidget(right_container, 2)
        self.set_stylesheet()
        self.stopped = False
        self.analysis_timer = QTimer()
        self.analysis_timer.timeout.connect(self.trigger_timed_analysis)
        # ------------------- å¯åŠ¨åå°æœåŠ¡ (åŠŸèƒ½æœªä¿®æ”¹) -------------------
        self.clear_log_files()
        self.timer = QTimer()
        self.timer.timeout.connect(self.update_tick)
        self.timer.start(33)
        self.logger.info("AI å¥èº«åŠ©æ‰‹ GUI å·²å¯åŠ¨")
        self.logger.info(f"é¦–æ¬¡å®šæ—¶åˆ†æå°†åœ¨å¯åŠ¨åç«‹å³å¼€å§‹,åç»­é—´éš” {TIMED_ANALYSIS_INTERVAL} ç§’ã€‚")
        QTimer.singleShot(1000, self.trigger_timed_analysis)
    def set_stylesheet(self):
        """è®¾ç½®åº”ç”¨çš„QSSæ ·å¼è¡¨"""
        style = """
        QMainWindow, #main_widget {
            background-color: #1e2a40;
        }
        QLabel {
            color: #d0d0d0;
            font-size: 14px;
        }
        QWidget#InfoPanel {
            background-color: rgba(38, 50, 70, 0.7);
            border-radius: 12px;
            border: 1px solid rgba(0, 255, 255, 0.2);
        }
        QLabel#PanelTitle {
            color: #ffffff;
            font-size: 15px;
            font-weight: bold;
            padding: 5px 12px;
            border-bottom: 1px solid rgba(0, 255, 255, 0.2);
        }
        #analysis_container, #summary_container {
            padding: 8px;
        }
        QTextEdit {
            background-color: transparent;
            border: none;
            color: #d0d0d0;
            font-size: 13px;
            padding: 0px 8px;
        }
        #camera_label {
            background-color: #000000;
            border: 2px solid #00ffff;
            border-radius: 12px;
            box-shadow: 0 0 15px #00ffff;
        }
        QPushButton {
            color: white;
            font-size: 14px;
            font-weight: bold;
            border-radius: 12px;
            padding: 8px 18px;
            border: 1px solid transparent;
            min-height: 36px;
        }
        QPushButton#AnalyzeButton {
            background-color: #00aaff;
        }
        QPushButton#AnalyzeButton:hover {
            background-color: #00cfff;
            border: 1px solid #ffffff;
        }
        QPushButton#StopButton {
            background-color: #ff4757;
        }
        QPushButton#StopButton:hover {
            background-color: #ff6b81;
            border: 1px solid #ffffff;
        }
        QPushButton:disabled {
            background-color: #505050;
            color: #a0a0a0;
        }
        """
        self.setStyleSheet(style)
    def setup_logging(self):
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',
                            handlers=[logging.FileHandler('ai_assistant_gui.log'), logging.StreamHandler()])
        self.logger = logging.getLogger(__name__)
    def clear_log_files(self):
        try:
            with open(self.llm_output_file, 'w', encoding='utf-8') as f:
                f.write(f"--- AI Assistant Log Initialized at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---\n\n")
            self.logger.info(f"å·²æ¸…ç©ºå¹¶åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶: {self.llm_output_file}")
        except Exception as e:
            self.logger.error(f"æ¸…ç©ºæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        try:
            with open(self.summary_output_file, 'w', encoding='utf-8') as f:
                f.write(f"--- Final Summary will be generated upon exit ---\n")
            self.logger.info(f"å·²æ¸…ç©ºå¹¶åˆå§‹åŒ–æ€»ç»“æ–‡ä»¶: {self.summary_output_file}")
        except Exception as e:
            self.logger.error(f"æ¸…ç©ºæ€»ç»“æ–‡ä»¶å¤±è´¥: {e}")
    def update_tick(self):
        if self.stopped:
            return
        ret, frame = self.camera.read()
        if ret:
            with self.frame_lock:
                self.current_frame = frame
            rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb_image.shape
            bytes_per_line = ch * w
            q_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
            pixmap = QPixmap.fromImage(q_image)
            self.camera_label.setPixmap(pixmap.scaled(self.camera_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation))
    def trigger_manual_analysis(self):
        if self.stopped:
            self.logger.info("åº”ç”¨å·²åœæ­¢,å¿½ç•¥æ‰‹åŠ¨åˆ†æè¯·æ±‚ã€‚")
            self.analysis_text.setPlainText("åº”ç”¨å·²åœæ­¢,æ— æ³•åˆ†æã€‚")
            return
        if self.analysis_timer.isActive():
            self.analysis_timer.stop()
            self.logger.info("å®šæ—¶åˆ†æå·²æš‚åœ,å¤„ç†æ‰‹åŠ¨æŒ‡ä»¤ã€‚")
        self.logger.info("--> 'å¼€å§‹åˆ†æ'æŒ‰é’®è¢«ç‚¹å‡», è¯»å–æœ€æ–°è¯­éŸ³æŒ‡ä»¤... <--")
        self.analysis_text.setPlainText("æ­£åœ¨è¯»å–è¯­éŸ³æŒ‡ä»¤...")
        voice_text = self.get_last_voice_command()
        if voice_text:
            self.logger.info(f"ğŸ¤ è¯»å–åˆ°æœ€æ–°è¯­éŸ³å‘½ä»¤: '{voice_text}'")
            processed_text = self.process_voice_command(voice_text)
            final_prompt = f"{processed_text}\n\nè¯·åŠ¡å¿…ç”¨ä¸­æ–‡å›ç­”ã€‚"
            self.logger.info(f"æ„é€ é€ç»™æ¨¡å‹çš„æœ€ç»ˆæç¤ºè¯: '{final_prompt}'")
            self.start_analysis_in_thread(final_prompt)
        else:
            self.logger.warning("è¯­éŸ³è¾“å…¥æ–‡ä»¶ä¸ºç©ºæˆ–æœªæ‰¾åˆ°,å°†ä½¿ç”¨é»˜è®¤æç¤ºè¯è¿›è¡Œåˆ†æã€‚")
            self.analysis_text.setPlainText("æœªæ£€æµ‹åˆ°è¯­éŸ³,ä½¿ç”¨é»˜è®¤åˆ†æã€‚")
            self.start_analysis_in_thread(self.analysis_prompt)
    def trigger_timed_analysis(self):
        if self.stopped:
            self.analysis_timer.stop()
            return
        self.logger.info(f"å®šæ—¶å™¨è§¦å‘,ä½¿ç”¨é»˜è®¤æç¤ºè¿›è¡Œåˆ†æ: '{self.analysis_prompt}'")
        self.analysis_text.setPlainText("æ­£åœ¨è¿›è¡Œå®šæ—¶è‡ªåŠ¨åˆ†æ...")
        self.start_analysis_in_thread(self.analysis_prompt)
    def start_analysis_in_thread(self, prompt):
        with self.frame_lock:
            if self.current_frame is None:
                self.logger.warning("è¯·æ±‚åˆ†æ,ä½†å½“å‰æ²¡æœ‰å¯ç”¨çš„æ‘„åƒå¤´å¸§ã€‚")
                return
            frame_to_analyze = self.current_frame.copy()
        self.analysis_text.setPlainText("åˆ†æä¸­ï¼Œè¯·ç¨å€™...")
        thread = threading.Thread(target=self.analysis_worker, args=(frame_to_analyze, prompt))
        thread.daemon = True
        thread.start()
    def analysis_worker(self, frame, prompt):
        response = self.query_vision_model(frame, prompt)
        if response:
            self.analysis_complete_signal.emit(response)
        else:
            self.logger.error("AI åˆ†æå¤±è´¥ã€‚")
            self.analysis_complete_signal.emit("AI åˆ†æå¤±è´¥ã€‚")
    def handle_analysis_result(self, result_text):
        if result_text.startswith("FINAL_SUMMARY:"):
            summary = result_text.replace("FINAL_SUMMARY:", "", 1)
            self.logger.info("åœ¨ GUI ä¸­æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“ã€‚")
            self.analysis_text.setPlainText("è®­ç»ƒæ€»ç»“å·²ç”Ÿæˆå®Œæ¯•ï¼")
            self.summary_text.setPlainText(summary)
            return
        self.logger.info(f"AI åˆ†æç»“æœ: {result_text}")
        self.analysis_text.setPlainText(f"{result_text}")
        self.save_llm_output(result_text)
        tts_thread = threading.Thread(target=self.text_to_speech, args=(result_text,))
        tts_thread.daemon = True
        tts_thread.start()
        if not self.stopped:
            self.logger.info(f"åˆ†æå®Œæˆ,å°†åœ¨ {TIMED_ANALYSIS_INTERVAL} ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡è‡ªåŠ¨åˆ†æã€‚")
            self.analysis_timer.start(TIMED_ANALYSIS_INTERVAL * 1000)
    def query_vision_model(self, frame, question):
        self.logger.info(f"æ­£åœ¨æŸ¥è¯¢è§†è§‰æ¨¡å‹, é—®é¢˜: {question}")
        cv2.imwrite(self.image_path, frame)
        try:
            with open(self.image_path, 'rb') as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            data = { "model": "RK3588-Qwen2-VL-2B", "messages": [ { "role": "user", "content": [ { "type": "image_url", "image_url": { "url": f"data:image/jpeg;base64,{base64_image}" } }, { "type": "text", "text": question } ] } ], "stream": True, "max_tokens": 1000 }
            response = requests.post(self.vision_api_url, headers={'Content-Type': 'application/json'}, json=data, stream=True, timeout=30)
            if response.status_code == 200:
                full_response = ""
                for chunk in response.iter_lines():
                    if chunk:
                        line = chunk.decode('utf-8')
                        if line.startswith('data: ') and not line.startswith('data: [DONE]'):
                            try:
                                json_data = json.loads(line[6:])
                                if json_data.get('choices') and len(json_data['choices']) > 0:
                                    delta = json_data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        content = delta['content']
                                        full_response += content
                            except json.JSONDecodeError:
                                continue
                return re.sub(r'[a-zA-Z0-9 .\-#*]', '', full_response.strip()) if full_response else None
            else:
                self.logger.error(f"è§†è§‰æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return f"è§†è§‰æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status_code}"
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢è§†è§‰æ¨¡å‹å¤±è´¥: {e}")
            return f"æŸ¥è¯¢è§†è§‰æ¨¡å‹æ—¶å‡ºé”™: {e}"
    def query_text_model(self, question):
        self.logger.info(f"æ­£åœ¨æŸ¥è¯¢æ–‡æœ¬æ¨¡å‹, é—®é¢˜: {question[:100]}...")
        try:
            data = { "model": "RK3588-Qwen2-VL-2B", "messages": [{"role": "user", "content": [{"type": "text", "text": question}]}], "stream": True, "max_tokens": 1500 }
            response = requests.post(self.vision_api_url, headers={'Content-Type': 'application/json'}, json=data, stream=True, timeout=120)
            if response.status_code == 200:
                full_response = ""
                for chunk in response.iter_lines():
                    if chunk:
                        line = chunk.decode('utf-8')
                        if line.startswith('data: ') and not line.startswith('data: [DONE]'):
                            try:
                                json_data = json.loads(line[6:])
                                if json_data.get('choices') and len(json_data['choices']) > 0:
                                    delta = json_data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        content = delta['content']
                                        full_response += content
                            except json.JSONDecodeError:
                                continue
                return re.sub(r'[a-zA-Z0-9 .\-#*]', '', full_response.strip()) if full_response else None
            else:
                self.logger.error(f"æ–‡æœ¬æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return f"æ–‡æœ¬æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status_code}"
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æ–‡æœ¬æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return f"æŸ¥è¯¢æ–‡æœ¬æ¨¡å‹æ—¶å‡ºé”™: {e}"
    def get_last_voice_command(self):
        try:
            if not os.path.exists(self.voice_output_file):
                self.logger.warning(f"è¯­éŸ³è¾“å…¥æ–‡ä»¶ {self.voice_output_file} ä¸å­˜åœ¨ã€‚")
                return None
            with open(self.voice_output_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            for line in reversed(lines):
                stripped_line = line.strip()
                if stripped_line:
                    return stripped_line
            self.logger.warning(f"è¯­éŸ³è¾“å…¥æ–‡ä»¶ {self.voice_output_file} ä¸ºç©ºã€‚")
            return None
        except Exception as e:
            self.logger.error(f"è¯»å–è¯­éŸ³æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None
    def process_voice_command(self, voice_text):
        for command, prompt in self.quick_commands.items():
            if command in voice_text:
                self.logger.info(f"ğŸ”„ åŒ¹é…åˆ°å¿«æ·å‘½ä»¤: {command}")
                return prompt
        return voice_text
    def text_to_speech(self, text):
        self.logger.info("å¼€å§‹è¿›è¡Œæ–‡å­—è½¬è¯­éŸ³...")
        try:
            cmd = [ TTS_BIN, f"--matcha-acoustic-model={TTS_ACOUSTIC_MODEL}", f"--matcha-vocoder={TTS_VOCODER}", f"--matcha-lexicon={TTS_LEXICON}", f"--matcha-tokens={TTS_TOKENS}", f"--matcha-dict-dir={TTS_DICT_DIR}", "--num-threads=2", f"--output-filename={self.output_audio_file}", text ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=45)
            if result.returncode == 0 and os.path.exists(self.output_audio_file):
                self.logger.info(f"TTSæˆåŠŸ,éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜åˆ°: {self.output_audio_file}")
                self.play_audio(self.output_audio_file)
            else:
                self.logger.error(f"TTSå¤±è´¥: {result.stderr}")
        except Exception as e:
            self.logger.error(f"TTSå¤„ç†å¤±è´¥: {e}")
    def play_audio(self, audio_file):
        self.logger.info(f"æ­£åœ¨ä½¿ç”¨ aplay æ’­æ”¾éŸ³é¢‘: {audio_file}")
        try:
            subprocess.run(['aplay', audio_file], check=True, capture_output=True, text=True, timeout=60)
            self.logger.info(f"éŸ³é¢‘æ’­æ”¾å®Œæ¯•: {audio_file}")
        except Exception as e:
            self.logger.error(f"æ’­æ”¾éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    def save_llm_output(self, text):
        self.logger.info("æ­£åœ¨ä¿å­˜å¤§æ¨¡å‹è¾“å‡º...")
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with open(self.llm_output_file, 'a', encoding='utf-8') as f:
                f.write(f"--- Response at {timestamp} ---\n{text}\n\n")
            self.logger.info(f"å¤§æ¨¡å‹è¾“å‡ºå·²æˆåŠŸè¿½åŠ åˆ°: {self.llm_output_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤§æ¨¡å‹è¾“å‡ºå¤±è´¥: {e}", exc_info=True)
    def stop_ai_services(self):
        self.logger.info("ç”¨æˆ·è¯·æ±‚åœæ­¢æœåŠ¡å¹¶ç”Ÿæˆæ€»ç»“ã€‚")
        self.running = False
        self.stopped = True
        if self.analysis_timer.isActive():
            self.analysis_timer.stop()
            self.logger.info("å®šæ—¶åˆ†æå·²åœæ­¢ã€‚")
        if self.camera.isOpened():
            self.camera.release()
            self.logger.info("æ‘„åƒå¤´èµ„æºå·²é‡Šæ”¾ã€‚")
        self.camera_label.setText("è®­ç»ƒå·²ç»“æŸ")
        self.camera_label.setStyleSheet(self.camera_label.styleSheet() + "color: white; font-size: 24px;")
        self.analysis_text.setPlainText("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€»ç»“...")
        self.analyze_now_button.setEnabled(False)
        self.stop_button.setEnabled(False)
        summary_thread = threading.Thread(target=self.perform_final_summary)
        summary_thread.daemon = True
        summary_thread.start()
    def perform_final_summary(self):
        self.logger.info("æ­£åœ¨å‡†å¤‡ç”Ÿæˆæœ€ç»ˆæ€»ç»“...")
        try:
            with open(self.llm_output_file, 'r', encoding='utf-8') as f:
                history = f.read()
            if len(history.splitlines()) < 3:
                self.logger.warning("è®­ç»ƒè®°å½•å¤ªå°‘,æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚")
                final_summary = "æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®æ¥ç”Ÿæˆæ€»ç»“ã€‚"
            else:
                summary_prompt = f"è¿™æ˜¯æœ¬æ¬¡å¥èº«çš„å…¨éƒ¨AIåˆ†æè®°å½•,è¯·æ ¹æ®è¿™äº›è®°å½•,ç”Ÿæˆä¸€ä»½å®Œæ•´çš„æœ€ç»ˆæ€»ç»“æŠ¥å‘Šã€‚æŠ¥å‘Šåº”åŒ…æ‹¬:1. æœ¬æ¬¡è®­ç»ƒæ•´ä½“è¡¨ç°çš„æ€»ç»“ã€‚2. åŠ¨ä½œçš„äº®ç‚¹å’Œä¸»è¦ä¼˜ç‚¹ã€‚3. æœ€éœ€è¦æ”¹è¿›çš„å‡ ä¸ªé—®é¢˜ç‚¹ã€‚4. é’ˆå¯¹é—®é¢˜ç‚¹çš„å…·ä½“ã€å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®ã€‚è¯·ä»¥æ˜“äºé˜…è¯»çš„æ ¼å¼å‘ˆç°,å›ç­”è¯·ä½¿ç”¨ä¸­æ–‡ã€‚è®°å½•å¦‚ä¸‹:\n\n{history}"
                final_summary = self.query_text_model(summary_prompt)
            if final_summary:
                self.logger.info("æˆåŠŸç”Ÿæˆæœ€ç»ˆæ€»ç»“ã€‚")
                self.save_final_summary(final_summary)
                self.analysis_complete_signal.emit(f"FINAL_SUMMARY:{final_summary}")
            else:
                self.logger.error("æœªèƒ½ç”Ÿæˆæœ€ç»ˆæ€»ç»“ã€‚")
                self.analysis_complete_signal.emit("FINAL_SUMMARY:ç”Ÿæˆæ€»ç»“å¤±è´¥,è¯·æ£€æŸ¥ç½‘ç»œæˆ–æ¨¡å‹æœåŠ¡ã€‚")
        except FileNotFoundError:
            self.logger.error(f"æ— æ³•æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ {self.llm_output_file} æ¥ç”Ÿæˆæ€»ç»“ã€‚")
            self.analysis_complete_signal.emit("FINAL_SUMMARY:æ‰¾ä¸åˆ°è®­ç»ƒè®°å½•æ–‡ä»¶,æ— æ³•ç”Ÿæˆæ€»ç»“ã€‚")
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ€»ç»“è¿‡ç¨‹ä¸­å‡ºç°æ„å¤–é”™è¯¯: {e}", exc_info=True)
            self.analysis_complete_signal.emit(f"FINAL_SUMMARY:ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™: {e}")
    def save_final_summary(self, text):
        self.logger.info("æ­£åœ¨ä¿å­˜æœ€ç»ˆæ€»ç»“...")
        try:
            with open(self.summary_output_file, 'w', encoding='utf-8') as f:
                f.write("--- AI Fitness Coach Final Summary ---\n")
                f.write(f"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(text)
            self.logger.info(f"æœ€ç»ˆæ€»ç»“å·²æˆåŠŸä¿å­˜åˆ°: {self.summary_output_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœ€ç»ˆæ€»ç»“å¤±è´¥: {e}", exc_info=True)
    def closeEvent(self, event):
        self.logger.info("æ”¶åˆ°å…³é—­çª—å£è¯·æ±‚,å¼€å§‹æ¸…ç†èµ„æº...")
        self.running = False
        self.exit_requested = True
        if self.camera.isOpened():
            self.camera.release()
            self.logger.info("æ‘„åƒå¤´èµ„æºå·²é‡Šæ”¾ã€‚")
        self.logger.info("åº”ç”¨å·²å…³é—­ã€‚")
        event.accept()
def main():
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    app = QApplication(sys.argv)
    window = AIFitnessApp()
    window.showMaximized()
    sys.exit(app.exec_())
if __name__ == "__main__":
    main()